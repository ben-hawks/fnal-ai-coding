{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Using AI Tools to Code at the lab","text":"<p>This documentation is intended for developers at Fermilab to configure and use LLMs to assist in coding and related tasks. </p> <p>This whole site is a work in progress, and NOT official recommendations by the lab or it's leadership! This is meant as a simple way to share knowledge about these tools as it's developing. Check and follow the AI at work page for official guidance on tools to use at and around the lab.</p>"},{"location":"#using-tools-to-code-not-to-be-confused-with-tool-calling","title":"Using Tools to Code (not to be confused with \"Tool Calling\")","text":"<p>Generally, there are two main ways to interact with different Large Language Models (LLMs) and use them for coding. </p> <p>The first, most common, and recommended way (by me, Ben Hawks) is to use them through your IDE. This is done via a plugin, or via a dedicated IDE built around a specific tool. You can read more about these here: IDEs and IDE Plugins. I personally recommend this approach due to the increased capabilities that it enables, such as automatic file editing/creation, codebase context awareness, etc.  The downside of this approach is that you are also limited by which plugin(s) you choose, which are currently all somewhat new and not yet fully featured, or have different sets of features compared to each other. </p> <p>The other way is through different tools that may or may not be specific to coding. You can read more here: Other Coding Tools.</p> <p>As some disambiguation, you might sometimes hear about the term \"Tool Calling\" in regards to LLMs and coding tools. This usually refers to using \"MCP Servers\", where an LLM is using other tools that are made available through specific interfaces/APIs on behalf of a user to perform advanced \"agentic\" actions (such as running a terminal command).</p>"},{"location":"#models","title":"Models","text":"<p>All of these tools require some model (LLM) to actually power them under the hood. A majority of tools you find advertised are typically powered by Cloud Models, which tend to offer better performance at the trade off of cost (paying a subscription and/or per token used) and little to no guarantee that your data, code, prompts, and other interactions will not be kept private/used to train further models. In some cases, the usage of your data etc. to train a model is an explicit condition to use a given model or tool. Because of this, we must be very careful of what tools we use with what code/data at the lab. In most cases, its better to be safe than sorry and choose a tool that can use Fermilab Hosted Models, which run on lab infrastructure, and are therefor okay for more (see cybersecurity/AI at work for more information) data/code than the cloud hosted models. </p> <p>All this being said, it falls upon you, the user, to adhere to the proper policies, and exercise caution and best judgement while using these tools. I/Any other authors of this documentation are not responsible for misuse of anything mentioned here or beyond. Do your own research, consult with Cybersecurity, AICO, and/or line management first, and if it feels wrong, don't do it.</p>"},{"location":"Other%20Coding%20Tools/","title":"Other Coding Tools","text":"<p>[[Claude Code]] is a tool created by [[Anthropic]] that requires a subscription and uses Cloud Models. </p> <p>[[OpenWebUI]]</p>"},{"location":"IDE%20Plugins/Cline/","title":"Cline","text":"<ul> <li>Cline\u00a0- <ul> <li>Jetbrains Plugin</li> <li>VSCode Plugin</li> <li>CLI\u00a0(I have not used this myself, so ymmv and you'll have to figure out how to configure it, though I imagine a lot of settings will be similar to what I've put below)</li> </ul> </li> </ul> <p>Note that it'll ask you to login when you first install the plugin. This is optional and you can skip it by pressing \"Configure API Keys\" (or something similar, I forget exactly what the button says)</p>"},{"location":"IDE%20Plugins/Cline/#configuring-cline","title":"Configuring Cline","text":"<p>When configuring Cline, use the following settings: Under \"Settings (Cog wheel in the upper right corner of the side panel that pops open) -&gt; API Configuration\"</p> <p>Qwen3 Coder - Follow settings from Fermilab Hosted Models for API access, additionally:   * OpenAI compatible API Key:\u00a0MUST BE SET TO ANY RANDOM TEXT\u00a0(plugin requires it to be set, but we don't use api keys on this endpoint) - Request Timeout (ms):\u00a0<code>60000</code>\u00a0- Usually wont take this long for responses, but the ollama endpoint will take a bit to \"warm up\" when you start using it most of the time, or if it's unloading/loading the model. Once it's warmed up the response time is generally pretty reasonable.</p> <p>Alternatively, if you want to use a larger model that's not specifically tuned for coding:</p> <p>GPT OSS 120b - Follow settings from Fermilab Hosted Models for API access, additionally: </p> <ul> <li>OpenAI compatible API Key:\u00a0MUST BE SET TO ANY RANDOM TEXT\u00a0(plugin requires it to be set, but we don't use api keys on this endpoint)</li> <li>Under \"Model Configuration\"<ul> <li>Supports Images: Unchecked/Disabled - Not supported by gpt-oss:120b afaik</li> <li>Context Window Size:\u00a0<code>128000</code>\u00a0- Max for gpt-oss:120b</li> <li>Max Output Tokens:\u00a0<code>-1</code></li> <li>Temperature:\u00a0<code>0</code>\u00a0(Not relevant afaik)</li> </ul> </li> </ul> <p>Under \"Features\" Set the following: (a lot of these might be the default settings)</p> <ul> <li>Enable Checkpoints: Checked/Enabled</li> <li>Enable MCP Marketplace: Checked/Enabled (Optional, but can make things easier down the road. Dont worry about MCPs at this point, though)</li> <li>MCP Display Mode:\u00a0<code>Plain Text</code></li> <li>OpenAI Reasoning Effort (relevant if using\u00a0<code>gpt-oss:120b</code>\u00a0only): Medium (Recommended, you can play with this if you want. Lower reasoning is faster but less accurate/analytical, higher is slower but more accurate/analytical most of the time)</li> <li>Enable Scrict Plan Mode: Checked/Enabled</li> <li>Enable Focus Chain: Checked/Enabled</li> </ul> <p>If you're doing work where it's relevant, you can configure Cline to use a browser automatically under the \"Browser\" tab, but I have not played with this. The settings seem pretty self explanatory, though.Under \"Terminal\" Set the following (a lot of these might also be the default settings)  </p> <ul> <li>Default Terminal Profile:\u00a0<code>Bash</code>\u00a0if on Mac/Linux (probably) and\u00a0<code>Powershell</code>\u00a0if on windows. If you are using WSL as your terminal in your IDE, you can also set it to\u00a0<code>WSL Bash</code>\u00a0, but make sure the terminal integration itself is configured properly or things will fail.</li> <li>Shell Integration Timeout:\u00a0<code>4</code>\u00a0is a good default, try increasing if you're on a remote connection and are running into issues</li> <li>Enable Aggressive Terminal Reuse: Checked/Enabled - Disable if you're running into issues with the commands Cline runs, though. Might fix things, but not a guarantee</li> <li>Terminal Output Limit:\u00a0<code>500</code>\u00a0- You can increase if your commands output a lot of relevant info that you want to be included in the context, but it will also fill up your context window faster.</li> </ul> <p>Under \"General\" Set the following  </p> <ul> <li>Preferred Language:\u00a0<code>English</code>\u00a0(for the sake of the models being trained mostly on English)</li> <li>!! Allow error and usage reporting !!: Unchecked/Disabled - VERY IMPORTANT IF USING ON PRIVATE REPOS</li> </ul> <p>Once Configured, hit the blue \"Done\" button in the top right to return to the main interface.</p>"},{"location":"IDE%20Plugins/Cline/#using-cline","title":"Using Cline","text":"<ul> <li>There are two main modes Cline operates in: \"Plan\" and \"Act\"<ul> <li>Use \"Plan\" first to analyze your codebase and docs within your repo, work with it/iterate to refine the plan to your liking. Tell it to ask you questions in your prompt. Be very specific wherever you can be, you will get better results this way. Give it details about the environment you're working in if it's not documented in a file somewhere (Otherwise, it will typically try and look through your repo to figure out whats installed if a\u00a0<code>requirements.txt</code>\u00a0file or something similar exists).</li> <li>Use \"Act\" mode to start actually performic agentic actions. As soon as you toggle the mode switch to \"Act\", it will start executing the plan. You can always hit \"Cancel\" between steps and clarify/tell it to fix something if it's getting stuck in a loop, doing something wrong, or otherwise needs some help/is going down the wrong rabbit hole.<ul> <li>By default, Cline will ask your permission to do most \"unsafe\" actions first, like creating/editing a file, installing a package, deleting something, etc. That being said, it uses the LLM itself to determine if the action is \"Safe\" in the first place, so monitor it and stop it if need be. If you're using tools/mcp servers, you can give it permission to \"Auto-Approve\" certain tools as it tries to use them. You can also give it permission to Auto-Approve most file Read commands right above the text entry box. You can also auto approve edit commands, but I don't recommend this.</li> <li>When editing/creating a file, Cline will give you a side by side diff so you can inspect and edit the changes it's making. It will ask you to save or cancel the changes before it moves on to the next step. If you edit the file or cancel the changes, it will note that as feedback and usually try again taking the feedback into consideration. If you keep rejecting/canceling the changes, it will usually stop acting pretty quickly.</li> <li>You can also tell it to ask you questions in \"Act\" mode, which I've found very useful. The open models aren't great at doing this unprompted, so be sure to tell it to ask you questions in your prompt.</li> </ul> </li> </ul> </li> <li>You can add specific files/folders in your project as context to your prompt by hitting the \"@\" button under the text entry box. This is useful if you want to point it to some specific files so it doesn't have to try and guess/find the right files. That being said, a file list of the project you're in is included as part of the prompt under the hood, so if your project is structured/named well and according to standard conventions, it's pretty good at finding the right files in my experience. You can also add specific non project files to your prompt by pressing the \"+\" button that's next to the \"@\" button (I don't know if it can parse PDFs, though. Image support is model specific and not supported by gpt-oss:120b afaik)</li> <li>At the top of the interface, it'll show a progress bar that indicates how much of your \"context window\" (basically it's memory) has been used. Keep an eye on this, it can fill up fast at times.<ul> <li>If you're using a lot of your context window, you can click the little \"Compact Prompt\" button next to the progress bar at the top of the window showing how much of the context window that's used. This will use the LLM to summarize what it's done so far and reset the context window to just that summary.</li> </ul> </li> <li>Under the context window progress bar, there are little colored squares that indicate the history/chain of actions it's taken in the current task. You can click on the squares to scroll back to a given action, then hover over the dashed line and click \"restore\" if you want to rollback to the state your codebase and task was in at that point.<ul> <li>This uses git under the hood afaik, but sometimes struggles with really large projects. It'll give you a warning when you first start a task if it can't initialize the checkpoints properly, so keep that in mind.</li> </ul> </li> <li>If you're finished doing a specific task, I recommend starting a new chat/task fresh (Click the \"+\" button in the upper right corner of the pane) so that the LLM doesn't get confused/carry over stuff from the last task accidentally. You do have to tell it about your project and create a new plan from scratch (unless you use some other advanced features, which I can get into later), but its typically better to start clean occasionally in my experience. It has a lot fewer issues if you logically separate what you're doing into different chats/tasks than if you just continuously use the same session.</li> <li>Generally, you will have better luck with code quality and the LLM understanding things if your project is structured in a typical/standardized way, whatever makes sense for the project/language you're using. It also will do better with more popular languages (so it does great at Python, but probably not as good at VHDL/Verilog), so keep your expectations in line if you deviate from the norm with your project.</li> <li>If you have a lot of good documentation for the libraries your using/your project, I can go over how to use tools to fetch and parse that documentation so the LLM will have a better understanding of what's going on. Let me know if this is the case and I'll write something up/setup a meeting going over how to do this.</li> <li>If you have multiple plugins for AI features/assistants in your IDE, I'd recommend disabling all of them except for what you're using at the moment to avoid issues. The plugins are somewhat fragile at the moment. </li> </ul>"},{"location":"IDE%20Plugins/Cline/#final-notes","title":"Final Notes","text":"<p>Finally, these tools are still very new and very imperfect. This goes not only for the LLMs, but things like the plugins, MCP servers, and general guidance on how to use everything as well. Sadly, you should expect annoyances, incomplete documentation, and rapidly changing functionality at the moment. That being said, if you do run into issues, please open an issue in the relevant location (Cline specifically has a\u00a0github repo\u00a0and a\u00a0discord\u00a0where you can raise issues past the\u00a0docs)This hopefully should get you started using Cline/LLMs for coding. There's more to go over (MCP Servers/tools, Workflows and rules, advanced prompting, etc.), but I can go over those in the future/separate posts. I'm still somewhat new to this myself, so please share back any tips or features that you've figured out yourself</p>"},{"location":"IDE%20Plugins/Github%20Copilot/","title":"Github Copilot","text":"<p>In terms of an official stance on what it can be used with/on, Github Copilot is technically out for review with Cybersecurity. In practice, some folks at the lab are using Github Copilot with purely open source codebases. It's best to use caution and best judgment when it comes to using Github Copilot at the moment. </p> <p>It currently supports multiple models that are evolving and updating over time. These are primarily Cloud Models. If using these Github Copilot/these cloud models (even outside of Github Copilot), please exercise caution with what codebases you are using, assume that all information, code, prompts, and other data is being sent to Github/The model providers to further train and refine their models, and therefor is not appropriate to use with all codebases. </p>"},{"location":"IDE%20Plugins/Github%20Copilot/#access-via-github-education","title":"Access via Github Education","text":"<p>If you're a lab employee, you may get \"Free\" access to Github Copilot Pro via Github Education as an \"Educator\"/\"Faculty Member\". !! WHEN APPLYING/VERIFYING YOU MUST COVER UP YOUR EMPLOYEE ID NUMBER ON YOUR BADGE !! It may take up to 72 hours to get accepted/activate. Once activated, you must activate the Copilot Pro benefits via this link</p> <p>This will give you unlimited access to some models, and a certain amount of credits to use with \"Premium\" models. </p> <p>Github education does also come with some other benefits that are worth looking into, but aren't the focus of this documentation. </p>"},{"location":"IDE%20Plugins/IDEs%20and%20IDE%20Plugins/","title":"IDEs and IDE Plugins","text":"<p>There are many different IDE's and plugins that promise to be the perfect AI coding assistant/platform, but only a few that are actually useful, trustworthy, and have worthwhile features. I'd personally recommend sticking to the tools mentioned below (at least those that I dont explicitly warn against here)</p> <p>The TL;DR version of this page is, at the moment, that you should use a plugin with your preferred IDE instead of a dedicated IDE for primarily privacy reasons. Cline and Kilo Code are the most stable/full featured plugins for using Fermilab Hosted Models, and Github Copilot is the smoothest experience for using Cloud Models (as long as you're going through Github Copilot, you can't bring your own API key with the Github Copilot plugin). </p> <p>If you're using Vim/Emacs or similar, I sadly don't have any specific recommendations for you. Some of these tools also offer command line tools that might be useful for your use case, so if anything, it might be worth researching those. You might also want to look at Other Coding Tools. </p>"},{"location":"IDE%20Plugins/IDEs%20and%20IDE%20Plugins/#ide-plugins-vscode-jetbrains-ides","title":"IDE Plugins (VSCode &amp; Jetbrains IDEs)","text":"<p>Below is a table overviewing the primary features of each plugin that I've personally tried. For more information on each plugin, see the individual page for each plugin. The plugins that support local/Fermilab Hosted Models are generally no cost and/or open source unless otherwise mentioned.</p> Plugin VSCode Support Jetbrains Support Agentic Mode(s) Chat Autocomplete Tool Use (MCP Servers) Stable? Cloud Models? Local/FNAL Models? Github Copilot \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Cline \u2705 \u26a0\ufe0f (Unsure pricing model in 2026+) \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u2705 Kilo Code \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f (In Beta, VSCode Only, No local model support) \u2705 \u2705 \u2705 \u2705 [[Continue.dev]] \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u2705"},{"location":"IDE%20Plugins/IDEs%20and%20IDE%20Plugins/#dedicated-ides-not-reccomended","title":"Dedicated IDEs (Not Reccomended)","text":"<p>There are two dedicated AI-powered IDEs that are popular right now Cursor and Windsurf. Both of these provide deep integration and many powerful features, but are paid products that offer minimal local model support, and have concerns regarding privacy and use of code/data/prompts and interactions with models for training and other purposes. </p> <p>VSCode does also have native Github Copilot integration nowadays, so it's recommended to turn that off if you're using another plugin to avoid conflicts of features.</p>"},{"location":"IDE%20Plugins/Kilo%20Code/","title":"Kilo Code","text":"<p>Kilo Code is an open source fork of Cline and Roo Code. It supports Fermilab Hosted Models and Cloud Models, but this guide will focus on using Fermilab Hosted Models. Configuration and use of Kilo Code is very similar to Cline, but with some key differences and improvements. At the moment it is what I use and tend to recommend to users who want to use models hosted at the lab. </p> <p>This page, specifically the \"Using Kilo Code\" section, is a heavy work in progress. Check back regularly for updates!</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#links-first-time-configuration","title":"Links &amp; First time Configuration","text":"<ul> <li>Kilo Code Docs\u00a0- <ul> <li>Jetbrains Plugin</li> <li>VSCode Plugin</li> <li>CLI\u00a0(I have not used this myself, so ymmv and you'll have to figure out how to configure it, though I imagine a lot of settings will be similar to what I've put below)</li> </ul> </li> </ul> <p>Note that it'll ask you to login when you first install the plugin. This is optional and you can skip it by pressing \"Configure API Keys\" (or something similar, I forget exactly what the button says).</p> <p>You can also access settings at any time by hitting the little cog wheel icon in the upper righthand corner of the Kilo Code pane/tab.  (Third from the right in the image below)  </p> <p>Read through all of the settings explanations here before using the plugin, especially \"About Kilo Code Configuration\"!!! </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#model-access-configuration-providers-tab","title":"Model Access Configuration (Providers tab)","text":"<p>For most Fermilab Hosted Models, enter the API settings and other parameters mentioned on the linked page. You can configure multiple providers so that you can switch between different models/endpoints on the fly. </p> <p>For some common models, configure these settings beyond the defaults:</p> <ul> <li>Qwen3 Coder<ul> <li>API Key: Any random text (required by the plugin)</li> <li>Image Support: Unchecked/Disabled - Not supported</li> <li>Advanced -&gt; Error &amp; Repetition Limit: 5</li> </ul> </li> <li>GPT OSS 120b<ul> <li>API Key: Any random text (required by the plugin)</li> <li>Enable Reasoning Effort: Enabled<ul> <li>Model Reasoning Effort: Medium</li> </ul> </li> <li>Advanced -&gt; Error &amp; Repetition Limit: 5</li> </ul> </li> </ul>"},{"location":"IDE%20Plugins/Kilo%20Code/#auto-approve-configuration","title":"Auto-Approve Configuration","text":"<p>By default, Kilo Code will always automatically allow the LLM to perform certain agentic actions. I Highly recommend disabling \"Write\" as one of these actions. When \"Write\" is disabled, it will show a diff of the original code and the LLM generated code, asking you to edit/approve the changes before applying them. I'd also recommend disabling \"Execute\" actions by default, as that allows for automatic execution of command line applications. My auto approve settings look as follows:</p> <p></p>"},{"location":"IDE%20Plugins/Kilo%20Code/#browser-configuration","title":"Browser Configuration","text":"<p>Only relevant if you are using a browser for interacting with/testing whatever you're developing. Default settings are likely fine as far as I know. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#checkpoints-configuration","title":"Checkpoints Configuration","text":"<p>By default, Kilo Code will create (git based) checkpoints under the hood for almost every action it takes on your codebase. This can take up a large amount of disk space depending on how large your repositories are, and how many tasks you have. I recommend enabling \"Enable automatic task cleanup\" and configuring the retention settings to your liking to avoid using to much disk space. As the name implies, this does remove \"tasks\" (basically chat sessions and their checkpoints) over time, but you can set it so that favorited tasks are never deleted. </p> <p>If you work with large repositories, I'd also recommend increasing \"Checkpoint Initialization timeout\" from the default 15s, as it will typically take longer for large repositories. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#display-configuration","title":"Display Configuration","text":"<p>Configure this to your liking, but I typically keep the defaults. This just configures some options for how the chat interface looks/works. I'd highly recommend keeping the \"task timeline\" active if nothing else, as this is how you can quickly restore checkpoints if you have that enabled. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#autocomplete-configuration","title":"Autocomplete Configuration","text":"<p>This is only present on the VSCode plugin This does not work with Local/Fermilab Hosted Models at the moment</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#notifications-configuration","title":"Notifications Configuration","text":"<p>Kilo Code can send notifications when a task is completed/other important events, so that if you have your IDE minimized it can notify you when it needs some sort of interaction. Configure as desired. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#context-configuration","title":"Context Configuration","text":"<p>Generally, these settings control what and how much information is included in the LLMs context window/what's sent along with your prompt. You typically should find a balance between sending enough context that the LLM can effectively answer your questions and perform tasks correctly, but not so much that you quickly max out the (limited) context window of a given model. I won't go over specific settings here, but will say that I tend to use the default values and only tweak them if I run into issues. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#terminal-configuration","title":"Terminal Configuration","text":"<p>Here you can configure some settings on how the LLM interacts with the terminal. I use the defaults here, but you can change them if you run into issues with the LLM either not completing a command successfully, or understanding the context/output of a given command. As with the above context settings, you generally want to find a balance of how much information to send to the LLM. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#prompts-configuration","title":"Prompts Configuration","text":"<p>Unless you know what you're doing, leave these alone by default. These are the system prompts that Kilo Code uses under the hood when performing specific tasks. They are sent to the LLM as a part of whatever you are sending, and typically provide some baseline rules/instructions on how to perform a given task. To my knowledge, these tend to be tuned for Anthropic models, but have worked fine for me on the Fermilab Hosted Models that I use.  </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#experimental-configuration","title":"Experimental Configuration","text":"<p>Here be dragons.</p> <p>These are some experimental features that the plugin is currently working on. These change over time, and some can be useful. But, as the name implies, they are experimental and prone to bugs. I'd recommend not using any of these unless you're willing to deal with whatever consequences come from the feature catastrophically failing somehow. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#language-configuration","title":"Language Configuration","text":"<p>Whatever the default language you want Kilo Code to be in. As far as I know, does not effect the interactions with the LLM/default prompts used. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#mcp-servers","title":"MCP Servers","text":"<p>Here you can install and configure MCP Servers to use with Kilo Code. The plugin provides a nice UI and marketplace, but you can always edit the MCP configuration JSON file manually to install/tweak anything not supported by the UI. You can click the little \"here\" text to open the JSON MCP config file:  </p> <p>The \"Installed\" tab lets you enable and disable certain MCP servers if you want to (from your global and project specific MCP servers). Additionally, you can edit the global or project specific MCP config files by scrolling down and hitting the buttons:  </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#about-kilo-code-configuration-very-important","title":"\"About Kilo Code\" Configuration (VERY IMPORTANT)","text":"<p>You MUST disable \"Allow Error and Usage Reporting\" in this tab!! It is REQUIRED to use Kilo Code with any non-public code. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#using-kilo-code","title":"Using Kilo Code","text":"<p>This section is a work in progress.</p> <p>Kilo Code primarily operates in a very \"Agentic\" manner. This means that, by default, it will perform actions and tasks (using built in tools and/or MCP Servers) related to the query you ask it instead of just responding with text to a given query. Be very aware of this before you start using Kilo Code.</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#agentic-modes","title":"Agentic \"Modes\"","text":"<p>In the bottom left corner of the text box, you'll see a dropdown menu to select a \"Mode\":</p> <p> It expands into this:</p> <p> Kilo code supports multiple \"Modes\", where each uses a different system prompt to perform various specific tasks. In general, I recommend starting with \"Orchestrator\" mode, which will determine subtasks and delegate them to the relevant modes as required to complete your request. Each mode is somewhat self explanatory, and you can view the prompt that each is using in Settings -&gt; Prompts. </p> <p>Additionally, you can create new/custom modes by hitting the \"Edit...\" button, if so desired. This is an advanced feature, though, and I have not explored it myself. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#selecting-a-modelprovider","title":"Selecting a Model/Provider","text":"<p>If you have multiple models, or \"Providers\", configured, you can select which one you are currently using via the dropdown with the name you gave the provider in the left corner of the text entry box:</p> <p></p>"},{"location":"IDE%20Plugins/Kilo%20Code/#additional-context-attaching-files","title":"Additional Context (Attaching Files)","text":"<p>If you want to specifically reference a given file in your project, you can do so by attaching it to your query via the paperclip icon in the lower right of the text entry box: </p> <p></p> <p>As far as I know, this does not automatically ingest and convert documents (such as PDFs), but is meant for text/code files exclusively.  </p> <p>By default, Kilo Code will attempt to discover and include any relevant files in your project when queried by the LLM to do so. This is just a way to clarify what file to use specifically if ambiguous and/or improve results quality. </p>"},{"location":"IDE%20Plugins/Kilo%20Code/#codebase-indexing","title":"Codebase Indexing","text":"<p>This is an advanced feature that you can enable to improve result quality when working with a specific codebase. It requires that you have a Qdrant vector database setup somewhere to hold all of the indexed code, and an Embedding Model provider setup somewhere to embed/index the code. This is an advanced feature, if you're interested in using it, contact Burt Holzman about setting up/using a Qdrant and Embedding Model endpoint.  </p> <p>If you're using <code>nomic-embed-text:latest</code> as your embedding model, you must set the \"Model Dimension\" to <code>768</code>.</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#enhance-prompt-with-additional-context","title":"\"Enhance Prompt with Additional Context\"","text":"<p>The 'Enhance Prompt' button helps improve your prompt by providing additional context, clarification, or rephrasing. Try typing a prompt in the text box and clicking the button to see how it works.</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#kilo-code-rules-workflows","title":"Kilo Code Rules &amp; Workflows","text":"<p>Rules allow you to provide Kilo Code with instructions it should follow in all modes and for all prompts. They are a persistent way to include context and preferences for all conversations in your workspace or globally.\u00a0Docs</p>"},{"location":"IDE%20Plugins/Kilo%20Code/#general-tips-tricks","title":"General Tips &amp; Tricks","text":"<p>If you're finished doing a specific task, I recommend starting a new chat/task fresh (Click the \"+\" button in the upper right corner of the pane) so that the LLM doesn't get confused/carry over stuff from the last task accidentally. You do have to tell it about your project and create a new plan from scratch, but its typically better to start clean occasionally in my experience. It has a lot fewer issues if you logically separate what you're doing into different chats/tasks than if you just continuously use the same session.</p> <p>Generally, you will have better luck with code quality and the LLM understanding things if your project is structured in a typical/standardized way, whatever makes sense for the project/language you're using. It also will do better with more popular languages (so it does great at Python, but probably not as good at VHDL/Verilog), so keep your expectations in line if you deviate from the norm with your project.</p>"},{"location":"Models/Cloud%20Models/","title":"Cloud Models","text":"<p>[[Anthropic]] [[OpenAI]] [[Other]]</p>"},{"location":"Models/Fermilab%20Hosted%20Models/","title":"Fermilab Hosted Models","text":"<p>The lab hosts multiple Open Weight models on lab-owned infrastructure. These are split across two primary endpoints. Not all models are tuned for code generation, but those that are recommended for coding tasks will be listed in Bold. These models can be used in any of the IDEs and IDE Plugins that support local models (Specifically, anything that supports \"Ollama\" or \"OpenAI Compatible\" endpoints.)</p>"},{"location":"Models/Fermilab%20Hosted%20Models/#you-must-be-on-the-lab-network-on-site-or-vpn-to-access-these-modelsendpoints","title":"!!! You must be on the lab network (on-site or VPN) to access these models/endpoints. !!!","text":""},{"location":"Models/Fermilab%20Hosted%20Models/#models-more-coming-soon","title":"Models (more coming soon)","text":"<ul> <li>Qwen3 Coder (vllm) - <ul> <li>Endpoint: Contact Burt Holzman for Access</li> <li>Endpoint Type: <code>OpenAI Compatible</code></li> <li>Model Name on endpoint: <code>Qwen3-coder:latest</code></li> <li>Max Tokens: 256,000</li> <li>API Key: none (leave blank, or random text if required by plugin)</li> <li>Tool Usage?: Yes</li> </ul> </li> <li>GPT OSS 120b<ul> <li>Endpoint: Contact Burt Holzman for Access</li> <li>Endpoint Type: <code>OpenAI Compatible</code></li> <li>Model Name on endpoint: <code>gpt-oss:120b</code></li> <li>Max Tokens: 128,000</li> <li>API Key: none (leave blank, or random text if required by plugin)</li> <li>Tool Usage?: Yes</li> </ul> </li> </ul>"},{"location":"Tool%20Calling/MCP%20Servers/","title":"MCP Servers","text":"<p>MCP Servers are effectively interfaces for LLMs that provide access to different traditional tools and information so that they can function \"agentic-ly\" (perform actions on their own/on behalf of a user). Different IDEs and IDE Plugins have different schemes for configuring and installing MCP Servers, but this guide will outline the general way they're installed and configured for use with Coding tools.</p> <p>If you're interested in the nitty gritty of how to build MCP servers, I recommend HuggingFace's MCP Course</p>"},{"location":"Tool%20Calling/MCP%20Servers/#installing-mcp-servers","title":"Installing MCP Servers","text":"<p>MCP Servers (and/or their dependencies) either have to be installed on your system or installed on a remote system that your system can access. Most MCP servers that you want to use to develop with will be installed locally on your machine. </p> <p>There is no standardized marketplace/repository/central location for finding MCP servers. Some IDEs and IDE Plugins offer their own, and you can find some marketplaces that exist and/or integrate with given plugins, though. </p> <p>Most (if not all) MCP Servers will provide documentation on how to install them. Typically, they are either Node.js based, or python based. They require you to have an active/compatible installation for the MCP server to work. </p> <p>For Node.js based MCP Servers, I'd recommend using Node Version Manager (NVM), since in my experience, not all tools use the same version. This requires a bit of extra configuration in the MCP Config files, but in my opinion gives some greater flexibility. You're also free to install Node.js normally and just pay attention to dependencies and version requirements as you install MCP Servers.  </p> <p>For Python based MCP Servers, I'd recommend (as most plugins also do) using UV to manage the environments and install the servers. </p>"},{"location":"Tool%20Calling/MCP%20Servers/#configuring-mcp-servers","title":"Configuring MCP Servers","text":"<p>To use an MCP server with a given plugin/coding tool, you must typically configure it in a JSON file that corresponds to that given plugin/coding tool. The location, format, and options for these files tends to differ, annoyingly. Look at the different IDEs and IDE Plugins documentation for details on where/how to configure MCP Servers. </p> <p>Some tools/plugins also distinguish between \"Global\" and \"Project/Repo Specific\" MCP servers, so be sure to pay attention where you're installing a given MCP Server, and if that MCP server is meant to be used in a given location as well. </p> <p>Some MCP servers that are publicly available will offer public remote endpoints, so you don't have to install them locally, or otherwise rely on remote services. Be cautious when evaluating and installing MCP Servers if you plan on using them for non-public code, as like with most cloud models and some IDEs and plugins, they will use/retain data to further train models and/or improve their own tools. </p>"},{"location":"Tool%20Calling/MCP%20Servers/#example-installation-configuration-nodejs","title":"Example Installation &amp; Configuration - Node.js","text":"<p>As an example, I'll walk through the steps to install Context7 locally using NVM on a mac/linux environment, which is an MCP Server used to access publicly available documentation for various codebases. You only need to install NVM once per machine you're working on, and a given version of Node depending on the version used by the MCP Server. Pay attention to the <code>args</code> field, where we configure <code>nvm use &lt;version&gt;</code>, as it might differ across different MCP Servers. </p> <ol> <li>Install  Node Version Manager (NVM)</li> <li>Install Node version &gt;= v18.0.0 via NVM using <code>nvm install 18</code></li> <li>Configure your MCP server json file for your plugin/environment (specifics/location of the file depends on what plugin or other coding tool you're using. Look up the documentation for your plugin/coding tool for more details)</li> </ol> <pre><code>{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source ~/.nvm/nvm.sh &amp;&amp; nvm use 18 &amp;&amp; npx -y @upstash/context7-mcp\"\n      ],\n    }\n  }\n}\n</code></pre>"},{"location":"Tool%20Calling/MCP%20Servers/#useful-mcp-servers","title":"Useful MCP Servers","text":"<p>There's a lot of MCP servers of varying quality out there at the moment, especially since the protocol is basically brand new, so again, be careful with what MCP servers you install and use. Treat it like any other application/package you install on your machine.</p> <p>I've found a few useful ones myself, and will share them below</p> <ul> <li>Context7 - This is a great tool for accessing public documentation in a format that LLMs can easily use. The short version of how it works is that it indexes the documentation for a given repo/tool (you can search for what's indexed here, as well as add any publicly available docs to it's index), generates some context, Q&amp;A, Examples, and summaries of the documentation using an LLM (I believe Claude, but I'm not certain), and makes it available via the MCP server. I believe that it only collects information on public documentation, and that any requests sent to the server are simply to query if a given package that is requested by a user exists in the index, and if so will retrieve the documentation for it. I'd still proceed with caution if you do want to start using it with non-public code, and recommend doing more research before relying on it heavily. </li> <li>\"Official\" MCP Servers - Reference implementations of common features. Generally safe to use (as far as I know) when installed locally. <ul> <li>Everything - Reference / test server with prompts, resources, and tools.</li> <li>Fetch - Web content fetching and conversion for efficient LLM usage.</li> <li>Filesystem - Secure file operations with configurable access controls.</li> <li>Git - Tools to read, search, and manipulate Git repositories.</li> <li>Memory - Knowledge graph-based persistent memory system.</li> <li>Sequential Thinking - Dynamic and reflective problem-solving through thought sequences.</li> <li>Time - Time and timezone conversion capabilities.</li> </ul> </li> <li>Playwright - Allow for the LLM to interact with a browser, navigate pages, take screenshots of a webpage, etc. Installed locally this interacts with (by default) an installation of Chrome/Chromium, so just be conscious of the actions being taken by the LLM when using it.   </li> <li>Github MCP Server - Allows an LLM to interact with Github specific features, such as PRs and Issues, CI/CD pipelines, etc. beyond the reference implementation git MCP server mentioned above. </li> </ul>"}]}